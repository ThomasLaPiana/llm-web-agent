# Environment Configuration for LLM Web Agent

# Mistral Mode: "local" for dockerized Ollama service, "cloud" for Mistral API
MISTRAL_MODE=local

# For local mode - Ollama endpoint
MISTRAL_LOCAL_ENDPOINT=http://localhost:11434

# For cloud mode - Mistral API configuration
MISTRAL_API_ENDPOINT=https://api.mistral.ai/v1/chat/completions
MISTRAL_API_KEY=your_mistral_api_key_here

# Server configuration
PORT=3000
RUST_LOG=info

# Chrome/Browser configuration (optional)
CHROME_BIN=/usr/bin/chromium
CHROME_PATH=/usr/bin/chromium 